{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright ActionML, LLC under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# ActionML licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "import predictionio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "MAIN_DATA_PATH = \"./full-guide-meta-and-prefs/prefs/part-*\"\n",
    "META_DATA_PATH = \"./full-guide-meta-and-prefs/meta/*/*\"\n",
    "TEST_DATA_OUT = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "csvName = MAIN_DATA_PATH\n",
    "df = (sqlContext\n",
    "      .read\n",
    "      .format('com.databricks.spark.csv')\n",
    "      .options(header=False, delimiter='\\t', quote='@',inferSchema=False)\n",
    "      .load(csvName))\n",
    "\n",
    "df = df.dropDuplicates(['C0', 'C1', 'C2'])\n",
    "\n",
    "# Select only those users that has at least on \"fresh\" event\n",
    "good_users = (df.select(df.C0.alias('userId'), (df.C1 == \"fresh\").alias('hasFresh'))\n",
    "                .groupBy(\"userId\")\n",
    "                .agg({'hasFresh': 'max'})\n",
    "                .filter(\"max(hasFresh) = True\")\n",
    "                .select(\"userId\"))\n",
    "\n",
    "# Select only records from \"good users\"\n",
    "good_df = df.join(good_users, \n",
    "                  df[\"C0\"] == good_users[\"userId\"], \n",
    "                  how=\"left_outer\").filter(\"userId = C0\")\n",
    "\n",
    "good_df = good_df.select(good_df[\"userId\"], \n",
    "                         good_df[\"C1\"].alias(\"label\"), \n",
    "                         good_df[\"C2\"].alias(\"movieId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvNameMeta = META_DATA_PATH\n",
    "df_meta = (sqlContext\n",
    "      .read\n",
    "      .format('com.databricks.spark.csv')\n",
    "      .options(header=False, delimiter=',', quote='\"',inferSchema=False)\n",
    "      .load(csvNameMeta))\n",
    "\n",
    "df_meta = df_meta.dropDuplicates(['C0']) # Remove duplicate records with the same ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now join appropriate CF data and meta data frame \n",
    "jdf = good_df.join(df_meta, good_df[\"movieId\"] == df_meta[\"C0\"], how=\"left_outer\")\n",
    "jdf_like = jdf.filter(\"label = 'fresh'\")\n",
    "jdf_dislike = jdf.filter(\"label = 'rotten'\")\n",
    "\n",
    "seed = 92671\n",
    "traindf, testdf = jdf_like.randomSplit([0.8, 0.2], seed)\n",
    "full_traindf = traindf.unionAll(jdf_dislike).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test data has only fresh events\n",
    "data = testdf.select(\"userId\", \"movieId\").collect()\n",
    "\n",
    "with open(TEST_DATA_OUT, \"w\") as f:\n",
    "    for row in data:\n",
    "        f.write(row.userId.encode('utf-8'))\n",
    "        f.write(\",\")\n",
    "        f.write(row.movieId.encode('utf-8'))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Primary events (fresh)\n",
    "like_events = (full_traindf.filter(\"label = 'fresh'\")\n",
    "                  .select(\"userId\", \"movieId\")\n",
    "                  .map(lambda x: (x.userId, \"like\", x.movieId)))\n",
    "\n",
    "# Dislikes\n",
    "dislike_events = (full_traindf.filter(\"label = 'rotten'\")\n",
    "                  .select(\"userId\", \"movieId\")\n",
    "                  .map(lambda x: (x.userId, \"dislike\", x.movieId)))\n",
    "\n",
    "def srec(userId, toSplit, label, event):\n",
    "    \"\"\" Creates list of tuples with multiple possible values in\n",
    "    one field (like genre, etc.)\n",
    "    \"\"\"\n",
    "    out_event = \"like-\" + event if label == \"fresh\" else \"dislike-\" + event\n",
    "    if toSplit is None:\n",
    "        return []\n",
    "    else:\n",
    "        return [(userId, out_event, s.strip()) \n",
    "                for s in toSplit.split(\"\\t\") if s.strip != \"\"]\n",
    "        \n",
    "    \n",
    "genre_events = (full_traindf.select(\"userId\", \"label\", jdf[\"C6\"].alias(\"genres\"))\n",
    "                .flatMap(lambda x: srec(x.userId, x.genres, x.label, \"genre\")))\n",
    "\n",
    "director_events = (full_traindf.select(\"userId\", \"label\", jdf[\"C7\"].alias(\"director\"))\n",
    "                .flatMap(lambda x: srec(x.userId, x.director, x.label, \"director\")))\n",
    "\n",
    "writer_events = (full_traindf.select(\"userId\", \"label\", jdf[\"C8\"].alias(\"writer\"))\n",
    "                .flatMap(lambda x: srec(x.userId, x.writer, x.label, \"writer\")))\n",
    "\n",
    "cast_events = (full_traindf.select(\"userId\", \"label\", jdf[\"C9\"].alias(\"cast\"))\n",
    "                .flatMap(lambda x: srec(x.userId, x.cast, x.label, \"cast\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exporter = predictionio.FileExporter(file_name=\"train_events.json\")\n",
    "\n",
    "rdds = [like_events, dislike_events, genre_events, \n",
    "        director_events, writer_events, cast_events]\n",
    "\n",
    "for rdd in rdds:\n",
    "    for s in rdd.collect():\n",
    "        (user, event, item) = s\n",
    "        # To avoid empty target_entity_id or entity_id\n",
    "        user = user.strip()\n",
    "        item = item.strip()\n",
    "        if (user != \"\") and (item != \"\"):\n",
    "            exporter.create_event(\n",
    "                event=event,\n",
    "                entity_type=\"user\",\n",
    "                entity_id=user,\n",
    "                target_entity_type=\"item\",\n",
    "                target_entity_id=item,\n",
    "            )\n",
    "exporter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
